# Example: Ollama-Only Configuration (No Claude)
#
# Use this if you want to use only free, local models
# without any Claude API costs. All tasks route to Ollama.
#
# Trade-off: Complex tasks may have lower quality output,
# but there's zero API cost.

conductor:
  classifier_model: ollama:qwen3:8b  # Use Ollama for classification too
  max_retries: 3
  validation_timeout: 300s

workers:
  claude:
    enabled: false  # Disable Claude entirely

  ollama:
    enabled: true
    endpoint: http://gpu-server.local:11434
    max_concurrent: 4
    models:
      # All tiers use Ollama
      fast: phi3:mini-16k
      default: qwen3:8b
      reasoning: deepseek-r1:32b  # Larger model for complex tasks

validators:
  pool_size: 3
  timeout: 120s
  backends:
    - ollama:qwen3:8b

ledger:
  path: .bigo/ledger.db

bus:
  buffer_size: 1000
